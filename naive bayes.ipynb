{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naive bayes is natural language processing 101\n",
    "#it is very naive and simple but powerful\n",
    "#quite similar to logistic regression i suppose\n",
    "#it is simply calculating posterior distribution from bayes theorem\n",
    "#pretty much the same as another generative learning algorithm gda\n",
    "#feel free to check gda in the following link\n",
    "# https://github.com/tattooday/machine-learning/blob/master/gaussian%20discriminant%20analysis.ipynb\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "import re\n",
    "os.chdir('d:/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a function to calculate top 30 frequent words\n",
    "#it would help us to determine which of those are stop words\n",
    "#stop words refer to the words we dont take in account for naive bayes\n",
    "#such as you, i, and, of, the, an\n",
    "#which are inevitable in both spam and ham\n",
    "#it is a stop word helper\n",
    "#we still need to filter out the words that dont belong in this list manually\n",
    "def get_stopword(output):\n",
    "    \n",
    "    vocabulary=sorted(list(set(output)))\n",
    "    \n",
    "    d={}\n",
    "    stopword=[]\n",
    "    \n",
    "    for i in vocabulary:\n",
    "        d[i]=output.count(i)\n",
    "    \n",
    "    #get top 30 frequent words\n",
    "    for j in sorted(list(d.values()))[::-1][:30]:\n",
    "        temp=list(d.keys())[list(d.values()).index(j)]\n",
    "        stopword.append(temp)\n",
    "        del d[temp]\n",
    "        \n",
    "    return stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#denote vocabulary as the list of all the words in english theoretically\n",
    "#in reality, it is not feasible and economic to do so\n",
    "#here, we just collect all words from all emails\n",
    "#we gotta filter out those stop words as well\n",
    "def get_vocabulary(output,stopword):\n",
    "    \n",
    "    vocabulary=sorted(list(set(output)))\n",
    "    \n",
    "    for i in vocabulary:\n",
    "        if i in stopword:\n",
    "            vocabulary.remove(i)\n",
    "        \n",
    "    return vocabulary    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate p(x|y=classification) for multivariate naive bayes\n",
    "def multivariate_calc_prob(word,x_train,y_train,classification):\n",
    "    \n",
    "    #how many spam or ham from all emails\n",
    "    num=list(y_train).count(classification)\n",
    "    \n",
    "    #check how many emails contain the given word\n",
    "    temp=[i.count(word) for i in x_train[y_train==classification]]\n",
    "    freq=len(temp)-temp.count(0)\n",
    "    \n",
    "    #calculate p(x|y=classification)\n",
    "    if freq!=0:\n",
    "        p=freq/num\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #laplace smoothing\n",
    "        #when the given word hasnt appeared in our training set yet\n",
    "        #we gotta avoid the scenario 0/num\n",
    "        #even if the given word has never appeared\n",
    "        #it doesnt indicate it wont appear in the future\n",
    "        #therefore, we gotta use laplace smoothing\n",
    "        #see the following link for more details\n",
    "        # https://en.wikipedia.org/wiki/Additive_smoothing\n",
    "        \n",
    "        #add 1 to the numerator and k to the denominator\n",
    "        #where k is the dimension of x\n",
    "        #in another word, how many possible values x can take\n",
    "        #in binary classification, k=2\n",
    "        #x either exists in the email or it doesnt\n",
    "        p=(freq+1)/(num+2)\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "#multivariate event model follows a bernoulli distribution\n",
    "#each word has two scenario\n",
    "#it either appears in the email or not\n",
    "def multivariate(sample,stopword):\n",
    "    \n",
    "    x_train,x_test,y_train,y_test= \\\n",
    "    train_test_split(sample['word'],sample['spam'],test_size=0.3)\n",
    "    \n",
    "    #caculate p(y)\n",
    "    phi_y0=list(y_train).count(0)/len(y_train)\n",
    "    phi_y1=1-phi_y0\n",
    "    \n",
    "    forecast=[]\n",
    "    \n",
    "    #assume all x are independent\n",
    "    #each word has its own probability of occurence in emails\n",
    "    #the probability of occurence of those words are not correlated\n",
    "    #hence, p(x|y) for all x can be written as\n",
    "    #the product of all probabilities of x\n",
    "    #which is called chain rule in bayesian network\n",
    "    #see the following link for details\n",
    "    # https://en.wikipedia.org/wiki/Chain_rule_%28probability%29\n",
    "    for i in x_test:\n",
    "        px_y0,px_y1=1,1\n",
    "        for j in i:\n",
    "            if j not in stopword:\n",
    "                px_y0*=multivariate_calc_prob(j,x_train,y_train,0)\n",
    "                px_y1*=multivariate_calc_prob(j,x_train,y_train,1)\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        #calculate p(y|x)\n",
    "        py0_x=px_y0*phi_y0\n",
    "        py1_x=px_y1*phi_y1\n",
    "        \n",
    "        #compare p(y=1|x) with p(y=0|x)\n",
    "        #we take the larger one as for all generative learning algo\n",
    "        #if the probabilities of spam and ham are equal\n",
    "        #we d rather see it in inbox folder instead of spam folder\n",
    "        p=0 if py0_x>=py1_x else 1\n",
    "        forecast.append(p)\n",
    "        \n",
    "    return len(y_test[forecast==y_test])/len(y_test)*100\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate p(x|y) for multinominal naive bayes\n",
    "def multinominal_calc_prob(word,x_train,y_train,classification,vocabulary):\n",
    "    \n",
    "    #get the word count of all spam/ham emails\n",
    "    num=sum([len(i) for i in x_train[y_train==classification]])\n",
    "    \n",
    "    #get the frequency of given word in all spam/ham emails\n",
    "    freq=sum([i.count(word) for i in x_train[y_train==classification]])\n",
    "    \n",
    "\n",
    "    if freq!=0:\n",
    "        p=freq/num\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #laplace smoothing \n",
    "        #here x can take on v possible values\n",
    "        #where v is the length of vocabulary list\n",
    "        #we are assuming vocabulary list covers every word in english\n",
    "        p=(freq+1)/(num+len(vocabulary))\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "\n",
    "#unlike multivariate, multinominal event model follows a multinominal distribution\n",
    "#the frequency of each word is taken into consideration\n",
    "#the formula is pretty much the same as multivariate\n",
    "#except p(x|y) is different\n",
    "def multinominal(sample,stopword):\n",
    "    \n",
    "    x_train,x_test,y_train,y_test= \\\n",
    "    train_test_split(sample['word'],sample['spam'],test_size=0.3)\n",
    "    \n",
    "    vocabulary=[]\n",
    "    for i in sample['word']:\n",
    "        temp=get_vocabulary(i,stopword)\n",
    "        vocabulary+=temp\n",
    "    \n",
    "    phi_y0=list(y_train).count(0)/len(y_train)\n",
    "    phi_y1=1-phi_y0\n",
    "    \n",
    "    forecast=[]\n",
    "    \n",
    "    for i in x_test:\n",
    "        px_y0,px_y1=1,1\n",
    "        for j in i:\n",
    "            if j not in stopword:\n",
    "                px_y0*=multinominal_calc_prob(j,x_train,y_train,0,vocabulary)\n",
    "                px_y1*=multinominal_calc_prob(j,x_train,y_train,1,vocabulary)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        py0_x=px_y0*phi_y0\n",
    "        py1_x=px_y1*phi_y1\n",
    "        \n",
    "        p=0 if py0_x>=py1_x else 1\n",
    "        forecast.append(p)\n",
    "        \n",
    "    return len(y_test[forecast==y_test])/len(y_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create multivariate vocabulary vector for sklearn\n",
    "#this is the authenticate way in naive bayes\n",
    "#but using pandas is more convenient\n",
    "#which is why our own implementation is kinda different\n",
    "def multivariate_vector(x,y,vocabulary):\n",
    "    \n",
    "    #CRUCIAL!!!!\n",
    "    #train_test_split would shuffle the training set\n",
    "    #the shuffled index would cause a problem later\n",
    "    x.reset_index(inplace=True,drop=True)\n",
    "    y.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    #we create a m*n matrix\n",
    "    #where m denotes the number of emails(rows)\n",
    "    #n denotes the number of words in vocabulary(columns)\n",
    "    #the value denotes if word[n] appears in email[m]\n",
    "    #if so, we set the value to 1\n",
    "    multivariate=pd.DataFrame()\n",
    "    for i in vocabulary:\n",
    "        temp=[]\n",
    "        for j in x.index:\n",
    "            if i in x[j]:\n",
    "                temp.append(1)\n",
    "            else:\n",
    "                temp.append(0)\n",
    "                \n",
    "        multivariate[i]=temp\n",
    "        \n",
    "    multivariate['real y']=y\n",
    "    \n",
    "    return multivariate\n",
    "\n",
    "\n",
    "#create multinominal vocabulary vector for sklearn\n",
    "def multinominal_vector(x,y,vocabulary):\n",
    "    \n",
    "    #CRUCIAL!!!!!\n",
    "    x.reset_index(inplace=True,drop=True)\n",
    "    y.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    #we create a m*n matrix\n",
    "    #where m denotes the number of emails(rows)\n",
    "    #n denotes the number of words in vocabulary(columns)\n",
    "    #the value denotes the frequency of word[n] in email[m]\n",
    "    multinominal=pd.DataFrame()\n",
    "    for i in vocabulary:\n",
    "        temp=[]\n",
    "        for j in x.index:\n",
    "            temp.append(x[j].count(i))\n",
    "                \n",
    "        multinominal[i]=temp\n",
    "        \n",
    "    multinominal['real y']=y\n",
    "    \n",
    "    return multinominal\n",
    "\n",
    "\n",
    "#using the official sklearn package\n",
    "def sklearn(sample,stopword,method,nbmethod):\n",
    "    \n",
    "    vocabulary=[]\n",
    "    for i in sample['word']:\n",
    "        temp=get_vocabulary(i,stopword)\n",
    "        vocabulary+=temp\n",
    "    \n",
    "    x_train,x_test,y_train,y_test= \\\n",
    "    train_test_split(sample['word'],sample['spam'],test_size=0.3)\n",
    "\n",
    "    mn_train=method(x_train,y_train,vocabulary)\n",
    "    mn_test=method(x_test,y_test,vocabulary)\n",
    "    \n",
    "    model=nbmethod().fit(pd.concat([mn_train[i] for i in vocabulary], \\\n",
    "                                      axis=1),mn_train['real y'])\n",
    "    \n",
    "    forecast=model.predict(pd.concat([mn_test[i] for i in vocabulary], \\\n",
    "                                      axis=1))\n",
    "    \n",
    "    return len(y_test[forecast==y_test])/len(y_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    #the raw data really comes from my email\n",
    "    df=pd.read_csv('spam.csv')\n",
    "    \n",
    "    #deaggregation splits the emails into lists of words\n",
    "    #output denotes the list of every word in all emails\n",
    "    deaggregation=[]\n",
    "    output=[]\n",
    "    for i in df['text']:\n",
    "        regex=re.findall('\\w*',i.lower())\n",
    "        temp=list(filter(lambda x: x!='',regex))\n",
    "        deaggregation.append(temp)\n",
    "        output+=temp\n",
    "        \n",
    "    df['word']=deaggregation\n",
    "    stopword=get_stopword(output)\n",
    "    \n",
    "    #some adjustment on stopword\n",
    "    stopword.remove('account')\n",
    "    stopword.remove('application')\n",
    "    stopword.remove('data')\n",
    "    stopword.remove('trip')\n",
    "\n",
    "    temp=[]\n",
    "    for i in range(20):\n",
    "        \n",
    "        temp.append(multivariate(df,stopword))\n",
    "        temp.append(multinominal(df,stopword))\n",
    "        temp.append(sklearn(df,stopword,multivariate_vector,BernoulliNB))\n",
    "        temp.append(sklearn(df,stopword,multinominal_vector,MultinomialNB))\n",
    "    \n",
    "    for j,k in enumerate(['multivariate implementation','multinominal implementation', \\\n",
    "                         'multivariate sklearn','multinominal sklearn']):\n",
    "        print('{} accuracy: {}%'.format(k,np.mean(temp[j::4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multivariate implementation accuracy: 80.0%\n",
      "multinominal implementation accuracy: 78.0%\n",
      "multivariate sklearn accuracy: 75.0%\n",
      "multinominal sklearn accuracy: 78.0%\n"
     ]
    }
   ],
   "source": [
    "main()\n",
    "\n",
    "#surprisingly the accuracy of my implementation is actually higher than sklearn\n",
    "#but it is a huge tradeoff for time\n",
    "#i would definitely use sklearn instead\n",
    "#btw, my sample size is very small\n",
    "#the result is definitely biased\n",
    "#it heavily depends on how train and test sets are splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
