{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naïve bayes is natural language processing 101\n",
    "#it is very naïve and simple but powerful\n",
    "#quite similar to logistic regression i suppose\n",
    "#it is simply calculating posterior distribution from bayes theorem\n",
    "#pretty much the same as another generative learning algorithm gda\n",
    "#feel free to check gda in the following link\n",
    "# https://github.com/je-suis-tm/machine-learning/blob/master/gaussian%20discriminant%20analysis.ipynb\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "os.chdir('h:/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert text into a list of words\n",
    "#we use stemming and lemmatization to save space and improve efficiency\n",
    "#for instance, we have words walked,walking,walks\n",
    "#with nltk package, we can revert all of them to walk\n",
    "def text2list(text,stopword,lower=True):\n",
    "\n",
    "    temp=text if lower==False else text.lower()\n",
    "    tokenizer=RegexpTokenizer(r'\\w+')\n",
    "    temp2=[WordNetLemmatizer().lemmatize(i) for i in tokenizer.tokenize(temp)]\n",
    "    output=[PorterStemmer().stem(i) for i in temp2 if i not in stopword]\n",
    "    \n",
    "    #remove numbers as they are stopword as well\n",
    "    for i in output:\n",
    "        try:\n",
    "            float(i)\n",
    "            output.remove(i)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a function to calculate top 30 frequent words\n",
    "#it would help us to determine which of those are stop words\n",
    "#stop words refer to the words we dont take in account for naïve bayes\n",
    "#such as you, i, and, of, the, an\n",
    "#which are inevitable in both spam and ham\n",
    "#it is a stop word helper\n",
    "#we still need to filter out the words that dont belong in this list manually\n",
    "#for each task, stop word may be different\n",
    "#e.g. if every text you need to classify has a prefix like '[Confidential]'\n",
    "#confidential becomes a stop word to be excluded\n",
    "def get_stopword(output):\n",
    "    \n",
    "    vocabulary=sorted(list(set(output)))\n",
    "    \n",
    "    d={}\n",
    "    stopword=[]\n",
    "    \n",
    "    for i in vocabulary:\n",
    "        d[i]=output.count(i)\n",
    "    \n",
    "    #get top 30 frequent words\n",
    "    for j in sorted(list(d.values()))[::-1][:30]:\n",
    "        temp=list(d.keys())[list(d.values()).index(j)]\n",
    "        stopword.append(temp)\n",
    "        del d[temp]\n",
    "        \n",
    "    return stopword\n",
    "\n",
    "\n",
    "#there is another way to find stop words called kl divergence\n",
    "#it calculates the distance from one distribution to another\n",
    "#in layman’s terms, it gives us the impact of each word on classifications\n",
    "#you can check Wikipedia for more details\n",
    "# https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence\n",
    "#it is also called mutual information in machine learning\n",
    "#the actual formula is kl(p(x:y)||p(x)p(y))\n",
    "#we want the divergence from xy joint distribution to xy independent distribution\n",
    "#in scipy, u can use scipy.stats.entropy\n",
    "#in sklearn, u can use sklearn.feature_selection.mutual_info_classif\n",
    "def kl_distance(df):\n",
    "\n",
    "    #concatenate all words together\n",
    "    temp=set([j for i in df['word'] for j in i])\n",
    "\n",
    "    #probability of y is fixed\n",
    "    #they are calculated before everything\n",
    "    py0=len(df[df['spam']==0])/len(df)\n",
    "    py1=len(df[df['spam']==1])/len(df)\n",
    "\n",
    "    #calculate kl distance for each word\n",
    "    temp2=[]\n",
    "    for word in set(temp):\n",
    "\n",
    "        kldistance=0\n",
    "\n",
    "        countx=[]\n",
    "\n",
    "        #to make our life easier\n",
    "        #we take multivariate approach\n",
    "        #we dont calculate the frequency of a word\n",
    "        for j in range(len(df)):\n",
    "            if df['word'][j].count(word)>0:\n",
    "                countx.append(1)\n",
    "            else:\n",
    "                countx.append(0)\n",
    "\n",
    "        #using dataframe even though the performance is slower\n",
    "        #it is easier to do slicing by logical expression\n",
    "        temp3=pd.DataFrame()\n",
    "        temp3['x']=countx\n",
    "        temp3['y']=df['spam']\n",
    "\n",
    "        px0=len(temp3[temp3['x']==0])/len(temp3)\n",
    "        px1=len(temp3[temp3['x']==1])/len(temp3)\n",
    "\n",
    "        #using loops to do sum\n",
    "        for k in range(2):\n",
    "            for l in range(2):\n",
    "                prob_joint=len(temp3[temp3['y']==k][temp3['x']==l])/len(temp3)\n",
    "\n",
    "                probx=px0 if l==0 else px1\n",
    "                proby=py0 if k==0 else py1\n",
    "\n",
    "                #we need to avoid 2 traps\n",
    "                #zero division and logarithm zero\n",
    "                try:\n",
    "                    entropy=prob_joint*np.log(prob_joint/(probx*proby))\n",
    "                except ZeroDivisionError:\n",
    "                    entropy=0\n",
    "\n",
    "                if prob_joint==0:\n",
    "                    entropy=0\n",
    "\n",
    "                kldistance+=entropy\n",
    "\n",
    "        temp2+=[word,kldistance]\n",
    "\n",
    "    #transform into dataframe and sort by kl distance in descending order\n",
    "    output=pd.DataFrame()\n",
    "    output['word']=temp2[0::2]\n",
    "    output['kl distance']=temp2[1::2]\n",
    "\n",
    "    output.sort_values(by='kl distance',ascending=False,inplace=True)\n",
    "\n",
    "    output.reset_index(inplace=True,drop=True)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword=stopwords.words('english')+['u',\n",
    " 'beyond',\n",
    " 'within',\n",
    " 'around',\n",
    " 'would',\n",
    " 'b',\n",
    " 'c',\n",
    " 'e',\n",
    " 'f',\n",
    " 'g',\n",
    " 'h',\n",
    " 'j',\n",
    " 'k',\n",
    " 'l',\n",
    " 'n',\n",
    " 'p',\n",
    " 'q',\n",
    " 'r',\n",
    " 'u',\n",
    " 'v',\n",
    " 'w',\n",
    " 'x',\n",
    " 'z',\n",
    " 'first']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#denote vocabulary as the list of all the words in english theoretically\n",
    "#in reality, it is not feasible and economic to do so\n",
    "#here, we just collect all words from all emails\n",
    "#we gotta filter out those stop words as well\n",
    "def get_vocabulary(output,stopword):\n",
    "    \n",
    "    vocabulary=sorted(list(set(output)))\n",
    "    \n",
    "    for i in vocabulary:\n",
    "        if i in stopword:\n",
    "            vocabulary.remove(i)\n",
    "        \n",
    "    return vocabulary    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate p(x|y=classification) for multivariate naïve bayes\n",
    "def multivariate_calc_prob(word,x_train,y_train,classification):\n",
    "    \n",
    "    #how many spam or ham from all emails\n",
    "    num=list(y_train).count(classification)\n",
    "    \n",
    "    #check how many emails contain the given word\n",
    "    temp=[i.count(word) for i in x_train[y_train==classification]]\n",
    "    freq=len(temp)-temp.count(0)\n",
    "    \n",
    "    #calculate p(x|y=classification)\n",
    "    if freq!=0:\n",
    "        p=freq/num\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #laplace smoothing\n",
    "        #when the given word hasnt appeared in our training set yet\n",
    "        #we gotta avoid the scenario 0/num\n",
    "        #even if the given word has never appeared\n",
    "        #it doesnt indicate it wont appear in the future\n",
    "        #therefore, we gotta use laplace smoothing\n",
    "        #see the following link for more details\n",
    "        # https://en.wikipedia.org/wiki/Additive_smoothing\n",
    "        \n",
    "        #add 1 to the numerator and k to the denominator\n",
    "        #where k is the dimension of x\n",
    "        #in another word, how many possible values x can take\n",
    "        #in binary classification, k=2\n",
    "        #x either exists in the email or it doesnt\n",
    "        p=(freq+1)/(num+2)\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "#multivariate event model follows a bernoulli distribution\n",
    "#each word has two scenario\n",
    "#it either appears in the email or not\n",
    "def multivariate(sample,stopword):\n",
    "    \n",
    "    x_train,x_test,y_train,y_test= \\\n",
    "    train_test_split(sample['word'],sample['spam'],test_size=0.3)\n",
    "    \n",
    "    #calculate p(y)\n",
    "    phi_y0=list(y_train).count(0)/len(y_train)\n",
    "    phi_y1=1-phi_y0\n",
    "    \n",
    "    forecast=[]\n",
    "    \n",
    "    #assume all x are independent\n",
    "    #each word has its own probability of occurrence in emails\n",
    "    #the probability of occurrence of those words are not correlated\n",
    "    #hence, p(x|y) for all x can be written as\n",
    "    #the product of all probabilities of x\n",
    "    #which is called chain rule in bayesian network\n",
    "    #see the following link for details\n",
    "    # https://en.wikipedia.org/wiki/Chain_rule_%28probability%29\n",
    "    for i in x_test:\n",
    "        px_y0,px_y1=1,1\n",
    "        for j in i:\n",
    "            if j not in stopword:\n",
    "                px_y0*=multivariate_calc_prob(j,x_train,y_train,0)\n",
    "                px_y1*=multivariate_calc_prob(j,x_train,y_train,1)\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        #calculate p(y|x)\n",
    "        py0_x=px_y0*phi_y0\n",
    "        py1_x=px_y1*phi_y1\n",
    "        \n",
    "        #compare p(y=1|x) with p(y=0|x)\n",
    "        #we take the larger one as for all generative learning algo\n",
    "        #if the probabilities of spam and ham are equal\n",
    "        #we d rather see it in inbox folder instead of spam folder\n",
    "        p=0 if py0_x>=py1_x else 1\n",
    "        forecast.append(p)\n",
    "        \n",
    "    return len(y_test[forecast==y_test])/len(y_test)*100\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate p(x|y) for multinomial naïve bayes\n",
    "def multinomial_calc_prob(word,x_train,y_train,classification,vocabulary):\n",
    "    \n",
    "    #get the word count of all spam/ham emails\n",
    "    num=sum([len(i) for i in x_train[y_train==classification]])\n",
    "    \n",
    "    #get the frequency of given word in all spam/ham emails\n",
    "    freq=sum([i.count(word) for i in x_train[y_train==classification]])\n",
    "    \n",
    "\n",
    "    if freq!=0:\n",
    "        p=freq/num\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #laplace smoothing \n",
    "        #here x can take on v possible values\n",
    "        #where v is the length of vocabulary list\n",
    "        #we are assuming vocabulary list covers every word in english\n",
    "        p=(freq+1)/(num+len(vocabulary))\n",
    "\n",
    "    return p\n",
    "\n",
    "\n",
    "\n",
    "#unlike multivariate, multinomial event model follows a multinomial distribution\n",
    "#the frequency of each word is taken into consideration\n",
    "#the formula is pretty much the same as multivariate\n",
    "#except p(x|y) is different\n",
    "def multinomial(sample,stopword):\n",
    "    \n",
    "    x_train,x_test,y_train,y_test= \\\n",
    "    train_test_split(sample['word'],sample['spam'],test_size=0.3)\n",
    "    \n",
    "    vocabulary=[]\n",
    "    for i in sample['word']:\n",
    "        temp=get_vocabulary(i,stopword)\n",
    "        vocabulary+=temp\n",
    "    \n",
    "    phi_y0=list(y_train).count(0)/len(y_train)\n",
    "    phi_y1=1-phi_y0\n",
    "    \n",
    "    forecast=[]\n",
    "    \n",
    "    for i in x_test:\n",
    "        px_y0,px_y1=1,1\n",
    "        for j in i:\n",
    "            if j not in stopword:\n",
    "                px_y0*=multinomial_calc_prob(j,x_train,y_train,0,vocabulary)\n",
    "                px_y1*=multinomial_calc_prob(j,x_train,y_train,1,vocabulary)\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        py0_x=px_y0*phi_y0\n",
    "        py1_x=px_y1*phi_y1\n",
    "        \n",
    "        p=0 if py0_x>=py1_x else 1\n",
    "        forecast.append(p)\n",
    "        \n",
    "    return len(y_test[forecast==y_test])/len(y_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create multivariate vocabulary vector for sklearn\n",
    "#this is the authenticate way in naïve bayes\n",
    "#but using pandas is more convenient\n",
    "#which is why our own implementation is kinda different\n",
    "def multivariate_vector(x,y,vocabulary):\n",
    "    \n",
    "    #CRUCIAL!!!!\n",
    "    #train_test_split would shuffle the training set\n",
    "    #the shuffled index would cause a problem later\n",
    "    x.reset_index(inplace=True,drop=True)\n",
    "    y.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    #we create a m*n matrix\n",
    "    #where m denotes the number of emails(rows)\n",
    "    #n denotes the number of words in vocabulary(columns)\n",
    "    #the value denotes if word[n] appears in email[m]\n",
    "    #if so, we set the value to 1\n",
    "    multivariate=pd.DataFrame()\n",
    "    for i in vocabulary:\n",
    "        temp=[]\n",
    "        for j in x.index:\n",
    "            if i in x[j]:\n",
    "                temp.append(1)\n",
    "            else:\n",
    "                temp.append(0)\n",
    "                \n",
    "        multivariate[i]=temp\n",
    "        \n",
    "    multivariate['real y']=y\n",
    "    \n",
    "    return multivariate\n",
    "\n",
    "\n",
    "#create multinomial vocabulary vector for sklearn\n",
    "def multinomial_vector(x,y,vocabulary):\n",
    "    \n",
    "    #CRUCIAL!!!!!\n",
    "    x.reset_index(inplace=True,drop=True)\n",
    "    y.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    #we create a m*n matrix\n",
    "    #where m denotes the number of emails(rows)\n",
    "    #n denotes the number of words in vocabulary(columns)\n",
    "    #the value denotes the frequency of word[n] in email[m]\n",
    "    multinomial=pd.DataFrame()\n",
    "    for i in vocabulary:\n",
    "        temp=[]\n",
    "        for j in x.index:\n",
    "            temp.append(x[j].count(i))\n",
    "                \n",
    "        multinomial[i]=temp\n",
    "        \n",
    "    multinomial['real y']=y\n",
    "    \n",
    "    return multinomial\n",
    "\n",
    "\n",
    "#using the official sklearn package\n",
    "def sklearn(sample,stopword,method,nbmethod):\n",
    "    \n",
    "    vocabulary=[]\n",
    "    for i in sample['word']:\n",
    "        temp=get_vocabulary(i,stopword)\n",
    "        vocabulary+=temp\n",
    "    \n",
    "    x_train,x_test,y_train,y_test= \\\n",
    "    train_test_split(sample['word'],sample['spam'],test_size=0.3)\n",
    "\n",
    "    mn_train=method(x_train,y_train,vocabulary)\n",
    "    mn_test=method(x_test,y_test,vocabulary)\n",
    "    \n",
    "    model=nbmethod().fit(pd.concat([mn_train[i] for i in vocabulary], \\\n",
    "                                      axis=1),mn_train['real y'])\n",
    "    \n",
    "    forecast=model.predict(pd.concat([mn_test[i] for i in vocabulary], \\\n",
    "                                      axis=1))\n",
    "    \n",
    "    return len(y_test[forecast==y_test])/len(y_test)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the raw data really comes from my email\n",
    "df=pd.read_csv('spam.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "temp=[]\n",
    "for i in df['text'].tolist():\n",
    "    temp.append(text2list(i,stopword,lower=True))\n",
    "            \n",
    "df['word']=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trip', 'pleas', 'applic', 'data', 'organ', 'student', 'account', 'person', 'use', 'follow', 'password', 'univers', 'inform', 'wolfram', 'http', 'interest', 'intern', 'price', 'regard', 'see', 'time', 'contact', 'detail', 'event', 'note', 'privaci', 'protect', 'provid', 'question', 'thank']\n"
     ]
    }
   ],
   "source": [
    "#to review stopword\n",
    "print(get_stopword([j for i in temp for j in i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Terry.Mai\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:79: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "C:\\Users\\Terry.Mai\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:87: RuntimeWarning: divide by zero encountered in log\n",
      "C:\\Users\\Terry.Mai\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:87: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          word  kl distance\n",
      "0        invit     0.290305\n",
      "1       follow     0.290305\n",
      "2        offic     0.290305\n",
      "3         team     0.290305\n",
      "4       differ     0.215762\n",
      "5       ticket     0.215762\n",
      "6       provid     0.215762\n",
      "7        studi     0.215762\n",
      "8    eventbrit     0.215762\n",
      "9       friend     0.215762\n",
      "10        look     0.215762\n",
      "11          co     0.215762\n",
      "12       quick     0.215762\n",
      "13          hi     0.215762\n",
      "14     neither     0.215762\n",
      "15      famili     0.215762\n",
      "16     student     0.215762\n",
      "17      includ     0.215762\n",
      "18       might     0.215762\n",
      "19      travel     0.215762\n",
      "20       thank     0.215762\n",
      "21        love     0.215762\n",
      "22       hesit     0.215762\n",
      "23          uk     0.215762\n",
      "24     gabriel     0.215762\n",
      "25     everyon     0.215762\n",
      "26      member     0.215762\n",
      "27        trip     0.215762\n",
      "28       organ     0.215762\n",
      "29        link     0.215762\n",
      "..         ...          ...\n",
      "482       well     0.013025\n",
      "483    protect     0.013025\n",
      "484      unabl     0.013025\n",
      "485     receiv     0.013025\n",
      "486        use     0.009137\n",
      "487       time     0.009137\n",
      "488      pleas     0.007959\n",
      "489     regard     0.007959\n",
      "490     intern     0.007959\n",
      "491   interest     0.007959\n",
      "492         wa     0.000000\n",
      "493      howev     0.000000\n",
      "494     result     0.000000\n",
      "495    account     0.000000\n",
      "496     submit     0.000000\n",
      "497       free     0.000000\n",
      "498       meet     0.000000\n",
      "499       want     0.000000\n",
      "500    technic     0.000000\n",
      "501      place     0.000000\n",
      "502       dear     0.000000\n",
      "503     experi     0.000000\n",
      "504       site     0.000000\n",
      "505     retain     0.000000\n",
      "506         go     0.000000\n",
      "507      login     0.000000\n",
      "508     import     0.000000\n",
      "509       help     0.000000\n",
      "510     regist     0.000000\n",
      "511       need     0.000000\n",
      "\n",
      "[512 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#to check each word's contribution to the result\n",
    "print(kl_distance(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try ten times and compare the result of self implementation and sklearn\n",
    "temp=[]\n",
    "for i in range(10):\n",
    "        \n",
    "    temp.append(multivariate(df,stopword))\n",
    "    temp.append(multinomial(df,stopword))\n",
    "    temp.append(sklearn(df,stopword,multivariate_vector,BernoulliNB))\n",
    "    temp.append(sklearn(df,stopword,multinomial_vector,MultinomialNB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multivariate implementation accuracy: 84.0%\n",
      "multinomial implementation accuracy: 88.0%\n",
      "multivariate sklearn accuracy: 78.0%\n",
      "multinomial sklearn accuracy: 74.0%\n"
     ]
    }
   ],
   "source": [
    "for j,k in enumerate(['multivariate implementation','multinomial implementation',\n",
    "                        'multivariate sklearn','multinomial sklearn']):\n",
    "    print('{} accuracy: {}%'.format(k,np.mean(temp[j::4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#surprisingly the accuracy of my implementation is actually higher than sklearn\n",
    "#but it is a huge tradeoff for time\n",
    "#i would definitely use sklearn instead\n",
    "#btw, my sample size is very small\n",
    "#the result is definitely biased\n",
    "#it heavily depends on how train and test sets are split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
