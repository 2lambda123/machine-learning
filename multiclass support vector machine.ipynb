{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure you have read binary svm before moving onto multiclass\n",
    "# https://github.com/tattooday/machine-learning/blob/master/binary%20support%20vector%20machine.ipynb\n",
    "\n",
    "import cvxopt.solvers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "import os\n",
    "os.chdir('d:/python/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plz refer to binary svm for this function\n",
    "def binary_svm(x_train,y_train,kernel='linear',poly_constant=0.0,poly_power=1,gamma=5):\n",
    "\n",
    "    y_product=np.outer(y_train,y_train)\n",
    "    \n",
    "    if kernel=='linear':\n",
    "        x_product=np.outer(x_train,x_train)\n",
    "    elif kernel=='polynomial':\n",
    "        temp=np.outer(x_train,x_train)\n",
    "        x_product=np.apply_along_axis(lambda x:(x+poly_constant)**poly_power,0,temp.ravel()).reshape(temp.shape)\n",
    "    else:\n",
    "        temp=np.mat([i-j for j in x_train for i in x_train]).reshape(len(x_train),len(x_train))\n",
    "        x_product=np.apply_along_axis(lambda x:np.exp(-1*gamma*(np.linalg.norm(x))**2),0,temp.ravel()).reshape(temp.shape)\n",
    "    \n",
    "    P=cvxopt.matrix(x_product*y_product)\n",
    "    q=cvxopt.matrix(-1*np.ones(len(x_train)))\n",
    "    G=cvxopt.matrix(np.diag(-1 * np.ones(len(x_train))))\n",
    "    h=cvxopt.matrix(np.zeros(len(x_train)))\n",
    "    A=cvxopt.matrix(y_train,(1,len(x_train)))\n",
    "    b=cvxopt.matrix(0.0)\n",
    "\n",
    "    solution=cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "    alpha=pd.Series(solution['x'])\n",
    "    w=np.sum(alpha*y_train*x_train)\n",
    "\n",
    "    b=-(min(x_train[y_train==1.0]*w)+max(x_train[y_train==-1.0]*w))/2\n",
    "\n",
    "    return w,b    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, one vs one multiclass svm\n",
    "#given n classes, we do n*(n-1)/2 times binary classification as one vs one\n",
    "#we would obtain w and b for each binary classification\n",
    "#when we make a prediction, we use each w and b to get the classification\n",
    "#now that we have a classification list of n*(n-1)/2\n",
    "#we just select the value with the most frequency in the list\n",
    "#that would be our prediction, voila!\n",
    "def get_accuracy_ovo(train,test,**kwargs):\n",
    "    \n",
    "    #calculate w and b for each binary classification\n",
    "    multiclass=train['y'].drop_duplicates().tolist()\n",
    "    multiclass_params={}\n",
    "    for i in range(len(multiclass)):\n",
    "        for j in range(i+1,len(multiclass)):\n",
    "            data=copy.deepcopy(train)\n",
    "            temp=np.select([data['y']==multiclass[i],data['y']==multiclass[j]], \\\n",
    "                            [-1.0,1.0],default=0.0)\n",
    "            data['y']=temp\n",
    "            data=data[data['y']!=0.0]\n",
    "            multiclass_params['{},{}'.format(multiclass[i], \\\n",
    "                                             multiclass[j])]=binary_svm(data['x'], \\\n",
    "                                                                        data['y'], \\\n",
    "                                                                        **kwargs)\n",
    "            \n",
    "    result=[]\n",
    "    \n",
    "    #store all the predictions in one list\n",
    "    #and select the value with the most frequency in this list\n",
    "    predict=[]\n",
    "    for i in train['x']:\n",
    "        temp=[]\n",
    "        for j in multiclass_params:\n",
    "            w=multiclass_params[j][0]\n",
    "            b=multiclass_params[j][1]\n",
    "            value=np.sign(np.multiply(w,i)+b)\n",
    "            temp.append(j.split(',')[0] if value==-1.0 else j.split(',')[1])\n",
    "        \n",
    "        predict.append(max(set(temp), key=temp.count))\n",
    "        \n",
    "    predict=pd.Series(predict).apply(int)\n",
    "    result.append('train accuracy: %.2f'%(\n",
    "        len(predict[predict==train['y']])/len(predict)*100)+'%')\n",
    "    \n",
    "    \n",
    "    #kinda the same as training sample prediction\n",
    "    predict=[]\n",
    "    for i in test['x']:\n",
    "        temp=[]\n",
    "        for j in multiclass_params:\n",
    "            w=multiclass_params[j][0]\n",
    "            b=multiclass_params[j][1]\n",
    "            value=np.sign(np.multiply(w,i)+b)\n",
    "            temp.append(j.split(',')[0] if value==-1 else j.split(',')[1])\n",
    "            \n",
    "        predict.append(max(set(temp), key=temp.count))\n",
    "\n",
    "    predict=pd.Series(predict).apply(int)\n",
    "    \n",
    "    result.append('test accuracy: %.2f'%(\n",
    "        len(predict[predict==test['y']])/len(predict)*100)+'%')\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternatively, one vs rest multiclass svm\n",
    "#given n classes, we do n times binary classification as one vs rest\n",
    "#we would obtain w and b for each binary classification\n",
    "#when we make a prediction, we use each w and b to get the decision function value\n",
    "#we select the classifier with the maximum decision function value\n",
    "#that classifier would return +1.0 and we would take it as the result\n",
    "def get_accuracy_ovr(train,test,**kwargs):\n",
    "    \n",
    "    multiclass=train['y'].drop_duplicates()\n",
    "    multiclass_params={}\n",
    "    \n",
    "    #calculate w and b for each binary classification\n",
    "    for i in multiclass:\n",
    "        data=copy.deepcopy(train)\n",
    "        data['y']=np.where(data['y']==i,1.0,-1.0)\n",
    "        multiclass_params[i]=binary_svm(data['x'],data['y'],**kwargs)\n",
    "\n",
    "    result=[]\n",
    "        \n",
    "    #store all the decision function values in one list\n",
    "    #and select the classifier which gives the largest value\n",
    "    predict=[]\n",
    "    for i in train['x']:\n",
    "        max_value=float('-inf')\n",
    "        idx=0\n",
    "        for j in multiclass_params:\n",
    "            w=multiclass_params[j][0]\n",
    "            b=multiclass_params[j][1]\n",
    "            value=np.multiply(w,i)+b\n",
    "            if value>max_value:\n",
    "                max_value=value\n",
    "                idx=j\n",
    "    \n",
    "        predict.append(idx)\n",
    "    \n",
    "    predict=pd.Series(predict).apply(int)\n",
    "    result.append('train accuracy: %.2f'%(\n",
    "        len(predict[predict==train['y']])/len(predict)*100)+'%')\n",
    "    \n",
    "    #kinda the same as training sample prediction\n",
    "    predict=[]\n",
    "    for i in test['x']:\n",
    "        max_value=float('-inf')\n",
    "        idx=0\n",
    "        for j in multiclass_params:\n",
    "            w=multiclass_params[j][0]\n",
    "            b=multiclass_params[j][1]\n",
    "            value=np.multiply(w,i)+b\n",
    "            if value>max_value:\n",
    "                max_value=value\n",
    "                idx=j\n",
    "    \n",
    "        predict.append(idx)\n",
    "\n",
    "    predict=pd.Series(predict).apply(int)\n",
    "    result.append('test accuracy: %.2f'%(\n",
    "        len(predict[predict==test['y']])/len(predict)*100)+'%')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using official sklearn package with the same parameters\n",
    "def skl_multiclass_svm(x_train,x_test,y_train,y_test,**kwargs):\n",
    "    \n",
    "    m=SVC(**kwargs).fit(np.array(x_train).reshape(-1, 1), \\\n",
    "                        np.array(y_train).ravel())\n",
    "    \n",
    "    train=m.score(np.array(x_train).reshape(-1, 1), \\\n",
    "                  np.array(y_train).ravel())*100\n",
    "    test=m.score(np.array(x_test).reshape(-1, 1), \\\n",
    "                 np.array(y_test).ravel())*100\n",
    "    \n",
    "    print('\\ntrain accuracy: %s'%(train)+'%')\n",
    "    print('\\ntest accuracy: %s'%(test)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y']=np.select([df['type']=='Iris-setosa', \\\n",
    "                   df['type']=='Iris-versicolor', \\\n",
    "                   df['type']=='Iris-virginica'],[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for the simplicity, let us reduce the dimension of x to 1\n",
    "temp=pd.concat([df[i] for i in df.columns if 'length' in i or 'width' in i],axis=1)\n",
    "x=PCA(n_components=1).fit_transform(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.Series([x[i].item() for i in range(len(x))])\n",
    "y=df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crucial!!!!\n",
    "#or we would get errors in the next step\n",
    "x_test.reset_index(inplace=True,drop=True)\n",
    "y_test.reset_index(inplace=True,drop=True)\n",
    "x_train.reset_index(inplace=True,drop=True)\n",
    "y_train.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.DataFrame({'x':x_train,'y':y_train})\n",
    "test=pd.DataFrame({'x':x_test,'y':y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -8.6660e+01 -2.0944e+02  1e+02  6e-15  2e+00\n",
      " 1: -1.8258e+02 -1.8562e+02  3e+00  1e-14  1e+00\n",
      " 2: -1.1211e+04 -1.1213e+04  2e+00  2e-12  1e+00\n",
      " 3: -6.4122e+07 -6.4122e+07  1e+02  3e-09  1e+00\n",
      " 4: -3.9742e+10 -3.9742e+10  7e+04  4e-06  1e+00\n",
      "Terminated (singular KKT matrix).\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.4006e+01 -2.7977e+01  4e+02  2e+01  2e+00\n",
      " 1: -1.7856e+01 -1.2674e+01  1e+02  7e+00  6e-01\n",
      " 2: -9.4617e+00 -4.3958e+00  8e+01  3e+00  3e-01\n",
      " 3: -2.7741e-01 -1.5962e+00  3e+00  7e-02  7e-03\n",
      " 4: -4.7994e-01 -9.1325e-01  6e-01  9e-03  9e-04\n",
      " 5: -5.8552e-01 -9.7564e-01  5e-01  5e-03  5e-04\n",
      " 6: -8.8473e-01 -8.9714e-01  1e-02  3e-05  3e-06\n",
      " 7: -8.9440e-01 -8.9453e-01  1e-04  3e-07  3e-08\n",
      " 8: -8.9450e-01 -8.9450e-01  1e-06  3e-09  3e-10\n",
      " 9: -8.9450e-01 -8.9450e-01  1e-08  3e-11  3e-12\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.4180e+01 -9.5985e+01  3e+02  1e+01  2e+00\n",
      " 1: -1.0652e+02 -1.6244e+02  2e+02  1e+01  2e+00\n",
      " 2: -4.5255e+02 -6.4641e+02  3e+02  9e+00  1e+00\n",
      " 3: -1.8506e+03 -2.0640e+03  2e+02  7e+00  1e+00\n",
      " 4: -5.7123e+03 -6.2308e+03  5e+02  7e+00  1e+00\n",
      " 5: -1.3183e+04 -1.4250e+04  1e+03  7e+00  1e+00\n",
      " 6: -6.7890e+04 -7.1672e+04  4e+03  7e+00  1e+00\n",
      " 7: -3.1372e+05 -3.2786e+05  1e+04  7e+00  1e+00\n",
      " 8: -2.5856e+06 -2.6656e+06  8e+04  7e+00  1e+00\n",
      " 9: -6.0661e+07 -6.1271e+07  6e+05  7e+00  1e+00\n",
      "10: -5.7330e+09 -5.7370e+09  4e+06  6e+00  1e+00\n",
      "11: -4.1192e+11 -4.1219e+11  3e+08  6e+00  1e+00\n",
      "Terminated (singular KKT matrix).\n"
     ]
    }
   ],
   "source": [
    "ovr=get_accuracy_ovr(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -4.6082e+00 -8.8512e+00  2e+02  1e+01  2e+00\n",
      " 1: -4.8162e+00 -2.9306e+00  3e+01  2e+00  3e-01\n",
      " 2: -1.2799e-01 -1.5401e+00  1e+00  4e-15  3e-15\n",
      " 3: -5.2294e-01 -9.0210e-01  4e-01  7e-16  1e-15\n",
      " 4: -6.5269e-01 -9.6697e-01  3e-01  7e-16  8e-16\n",
      " 5: -8.8851e-01 -8.9732e-01  9e-03  5e-17  1e-15\n",
      " 6: -8.9444e-01 -8.9453e-01  9e-05  4e-16  1e-15\n",
      " 7: -8.9450e-01 -8.9450e-01  9e-07  4e-16  1e-15\n",
      "Optimal solution found.\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -2.8624e+01 -7.6697e+01  3e+02  2e+01  3e+00\n",
      " 1: -8.8730e+01 -1.7757e+02  3e+02  1e+01  2e+00\n",
      " 2: -5.2039e+02 -6.7558e+02  2e+02  8e+00  1e+00\n",
      " 3: -1.4043e+03 -1.6678e+03  3e+02  7e+00  1e+00\n",
      " 4: -2.2960e+03 -2.6846e+03  4e+02  7e+00  1e+00\n",
      " 5: -5.2025e+03 -5.9217e+03  7e+02  7e+00  1e+00\n",
      " 6: -5.4043e+03 -6.1478e+03  8e+02  7e+00  1e+00\n",
      " 7: -2.6778e+04 -2.8690e+04  2e+03  6e+00  1e+00\n",
      " 8: -2.5261e+05 -2.6048e+05  8e+03  6e+00  1e+00\n",
      " 9: -7.1411e+06 -7.1801e+06  4e+04  6e+00  1e+00\n",
      "10: -1.2582e+09 -1.2585e+09  3e+05  6e+00  1e+00\n",
      "11: -4.4101e+11 -4.4111e+11  1e+08  6e+00  1e+00\n",
      "12: -1.0408e+12 -1.0411e+12  2e+08  6e+00  1e+00\n",
      "13: -1.7880e+12 -1.7883e+12  4e+08  6e+00  1e+00\n",
      "14: -2.6690e+12 -2.6694e+12  4e+08  6e+00  1e+00\n",
      "Terminated (singular KKT matrix).\n",
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -3.0893e+00 -5.3966e+00  2e+02  1e+01  2e+00\n",
      " 1: -1.2747e+00 -8.8923e-01  2e+01  1e+00  1e-01\n",
      " 2:  9.5048e-03 -6.4006e-01  6e-01  4e-16  2e-15\n",
      " 3: -1.8122e-01 -2.7592e-01  9e-02  2e-17  6e-16\n",
      " 4: -2.0605e-01 -2.9421e-01  9e-02  2e-16  5e-16\n",
      " 5: -2.6637e-01 -2.7038e-01  4e-03  2e-16  7e-16\n",
      " 6: -2.7008e-01 -2.7012e-01  4e-05  2e-16  6e-16\n",
      " 7: -2.7012e-01 -2.7012e-01  4e-07  6e-17  6e-16\n",
      " 8: -2.7012e-01 -2.7012e-01  4e-09  2e-16  5e-16\n",
      "Optimal solution found.\n"
     ]
    }
   ],
   "source": [
    "ovo=get_accuracy_ovo(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one vs rest self implementation\n",
      "\n",
      " train accuracy: 60.00%\n",
      "\n",
      " test accuracy: 44.44%\n"
     ]
    }
   ],
   "source": [
    "print('one vs rest self implementation')\n",
    "for i in ovr:\n",
    "    print('\\n',i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one vs one self implementation\n",
      "\n",
      " train accuracy: 90.48%\n",
      "\n",
      " test accuracy: 95.56%\n"
     ]
    }
   ],
   "source": [
    "#normally ovo should work better than ovr \n",
    "#as time complexity of ovo is higher\n",
    "#n*(n-1)/2>n\n",
    "print('one vs one self implementation')\n",
    "for i in ovo:\n",
    "    print('\\n',i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one vs rest sklearn\n",
      "\n",
      "train accuracy: 95.23809523809523%\n",
      "\n",
      "test accuracy: 88.88888888888889%\n"
     ]
    }
   ],
   "source": [
    "print('one vs rest sklearn')\n",
    "skl_multiclass_svm(x_train,x_test,y_train,y_test,kernel='linear',decision_function_shape='ovr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one vs one sklearn\n",
      "\n",
      "train accuracy: 95.23809523809523%\n",
      "\n",
      "test accuracy: 88.88888888888889%\n"
     ]
    }
   ],
   "source": [
    "print('one vs one sklearn')\n",
    "skl_multiclass_svm(x_train,x_test,y_train,y_test,kernel='linear',decision_function_shape='ovo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
