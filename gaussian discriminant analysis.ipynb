{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gda is an interesting classification method\n",
    "#it is a generative learning algorithm based on probabilities\n",
    "#it requires a stronger assumption than logistic\n",
    "#to our surprise, logistic does not require any distribution assumption\n",
    "#not even exponential family distribution\n",
    "#thats why we should always try logistic first\n",
    "#for details of classification and discriminative algorithm such as logistic\n",
    "#plz refer to my previous jupyter notebook\n",
    "# https://github.com/tattooday/machine-learning/blob/master/newton%20method%20for%20logistic%20regression.ipynb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as lda\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.chdir('d:/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plz note all calculations involved are linear algebra\n",
    "#the sum of x**2 in matrix form is x'x where prime stands for transposed\n",
    "def gaussian_discriminant_analysis(y,x):\n",
    "    \n",
    "    #split test and train as usual\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
    "    \n",
    "    #denote phi as the probability of y==1\n",
    "    #we just calculate the number of samples where y==1\n",
    "    #divided by the total number samples\n",
    "    phi=len(x_train[y_train==1])/len(x_train)\n",
    "    \n",
    "    #calculate the 'mean vector' for two scenarios\n",
    "    #y==0 and y==1\n",
    "    #note that we use pandas dataframe\n",
    "    #so the results are not a vector\n",
    "    #and we dont need to transpose it\n",
    "    #cuz later when we need to input a tranposed vector\n",
    "    #we can directly input mean0/mean1\n",
    "    mean0=np.mean(x_train[y_train==0],0)\n",
    "    mean1=np.mean(x_train[y_train==1],0)\n",
    "    \n",
    "    #calculate the difference between x and mean for two scenarios\n",
    "    #concatnate x for two scenarios together\n",
    "    dif=pd.concat([x_train[y_train==0]-mean0,x_train[y_train==1]-mean1])\n",
    "    \n",
    "    #calculate the covariance matrix\n",
    "    #we use a list to append all covariance/variance\n",
    "    #later we reshape it into a 4 by 4 matrix\n",
    "    #our x is four-dimensional so we should have 4 by 4\n",
    "    temp=list(dif.columns)\n",
    "    cov=[]\n",
    "    \n",
    "    for i in range(len(dif.columns)):\n",
    "        for j in range(len(dif.columns)):\n",
    "            cov.append( \\\n",
    "                       ( \\\n",
    "                        np.mat(dif[temp[i]])*np.mat(dif[temp[j]]).T).item()/len(temp) \\\n",
    "                      )\n",
    "\n",
    "    cov=np.mat(cov).reshape(4,4)\n",
    "    \n",
    "    \n",
    "    #now we have mean for two scenarios,covariance matrix and phi\n",
    "    #we use bayesian conditional probability formula\n",
    "    #to calculate the probability of y==0 and y==1 for each x\n",
    "    #and we use the larger probability of the two to forecast y\n",
    "    p0=[]\n",
    "    p1=[]\n",
    "    \n",
    "    #the bayes' theorem is p(y|x)=p(x|y)*p(y)/p(x)\n",
    "    #in our case, x is always independent of y\n",
    "    #which gives us p(x)=1\n",
    "    #and we shall simplify it into p(y|x)=p(x|y)*p(y)\n",
    "    #where p(y)=phi**y+(1-phi)**(1-y) is the probability density function of bernoulli distribution\n",
    "    #p(x|y) is the pdf of multivariate gaussian distribution\n",
    "    #note that our pandas dataframe is not transposed\n",
    "    #so the formulas here are a lil bit different from the original\n",
    "    #plz refer to wikipedia for the details of formulas\n",
    "    # https://en.wikipedia.org/wiki/Bayes%27_theorem\n",
    "    # https://en.wikipedia.org/wiki/Multivariate_normal_distribution\n",
    "    # https://en.wikipedia.org/wiki/Covariance_matrix\n",
    "    # https://see.stanford.edu/materials/aimlcs229/cs229-notes2.pdf\n",
    "    \n",
    "    #we calculate the probability for each element in x matrix\n",
    "    #if we use the whole matrix to do algebra, lets see\n",
    "    #lets forget anything before np.e as we can see determinant there\n",
    "    #everything before np.e becomes scalar\n",
    "    #for (x-mean).T*cov.I*(x-mean)\n",
    "    #(x-mean).T is n by 4 matrix\n",
    "    #cov.I is 4 by 4 matrix\n",
    "    #we get n by 4 matrix\n",
    "    #then multiplited by (x-mean) which is 4 by n matrix\n",
    "    #we would end up with n by n matrix in the end\n",
    "    #oh, god, thats not what we want\n",
    "    #when (x-mean).T is 1 by 4 matrix\n",
    "    #after multiplication by cov.I which is 4 by 4 matrix\n",
    "    #now we get 1 by 4 matrix\n",
    "    #eventually times (x-mean).T which is 4 by 1 matrix\n",
    "    #we get scalar, a 1 by 1 matrix!!!\n",
    "    for k in range(len(x_test)):\n",
    "        \n",
    "        probability0=phi/( \\\n",
    "                          np.linalg.det(2*np.pi*cov)**(0.5) \\\n",
    "                         ) \\\n",
    "        *np.exp( \\\n",
    "                -0.5*(np.mat(x_test.iloc[k]-mean0))*cov.I*(np.mat(x_test.iloc[k]-mean0)).T \\\n",
    "               )\n",
    "        \n",
    "        \n",
    "        probability1=phi/( \\\n",
    "                          np.linalg.det(2*np.pi*cov)**(0.5) \\\n",
    "                         ) \\\n",
    "        *np.exp( \\\n",
    "                -0.5*(np.mat(x_test.iloc[k]-mean1))*cov.I*(np.mat(x_test.iloc[k]-mean1)).T \\\n",
    "               )\n",
    "        \n",
    "        \n",
    "        p0.append(probability0.item())\n",
    "        p1.append(probability1.item())\n",
    "    \n",
    "    #here we use numpy sign\n",
    "    #numpy sign treats positive number as 1\n",
    "    #it treats zero as 0\n",
    "    #however, it treats negative number as -1\n",
    "    #we use a map function to convert -1 to 0\n",
    "    forecast=np.sign(np.subtract(p1,p0))\n",
    "    forecast=list(map(lambda x: 0 if x<0 else int(x),forecast))\n",
    "    \n",
    "    print('test accuracy: {}%'.format(len(y_test[forecast==y_test])/len(y_test)*100))\n",
    "    \n",
    "    #just too lazy to write codes for plotting\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear discriminant analysis from sklearn\n",
    "def LDA(y,x):\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
    "    \n",
    "    m=lda().fit(x_train,y_train)\n",
    "    \n",
    "    forecast=m.predict(x_test)\n",
    "    \n",
    "    print('test accuracy: {}%'.format(len(y_test[forecast==y_test])/len(y_test)*100))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#iris data manipulation as usual\n",
    "#simplify data into a binary classification problem\n",
    "df=pd.read_csv('iris.csv')\n",
    "df=df[df['type']!='Iris-versicolor']\n",
    "df['y']=np.select([df['type']=='Iris-setosa',df['type']=='Iris-virginica'],[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare x and y\n",
    "x=pd.concat([df[i] for i in df.columns[:4]],axis=1)\n",
    "y=df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "gaussian_discriminant_analysis(y,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "LDA(y,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
